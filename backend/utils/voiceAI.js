// utils/voiceAI.js - Voice-first interaction using Vertex AI Speech services
import speech from '@google-cloud/speech';
import textToSpeech from '@google-cloud/text-to-speech';
import { generativeModel } from './vertex.js';
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';
import dotenv from 'dotenv';

dotenv.config();

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Initialize Speech-to-Text client
const speechClient = new speech.SpeechClient();

// Initialize Text-to-Speech client
const ttsClient = new textToSpeech.TextToSpeechClient();

// Supported languages/dialects for Indian farmers
const SUPPORTED_LANGUAGES = {
  'hi-IN': 'Hindi',
  'en-IN': 'English (India)',
  'bn-IN': 'Bengali',
  'te-IN': 'Telugu',
  'mr-IN': 'Marathi',
  'ta-IN': 'Tamil',
  'gu-IN': 'Gujarati',
  'kn-IN': 'Kannada',
  'ml-IN': 'Malayalam',
  'pa-IN': 'Punjabi',
  'or-IN': 'Odia'
};

/**
 * Convert speech audio buffer to text using Vertex AI Speech-to-Text
 * @param {Buffer} audioBuffer - Audio file buffer
 * @param {string} languageCode - Language code (e.g., 'hi-IN', 'en-IN')
 * @param {string} encoding - Audio encoding (e.g., 'WEBM_OPUS', 'MP3', 'WAV')
 * @param {number} sampleRateHertz - Sample rate of audio
 * @returns {Promise<string>} Transcribed text
 */
async function speechToText(audioBuffer, languageCode = 'hi-IN', encoding = 'WEBM_OPUS', sampleRateHertz = 48000) {
  try {
    // Enhanced request configuration for better recognition
    const request = {
      audio: {
        content: audioBuffer.toString('base64'),
      },
      config: {
        encoding: encoding,
        sampleRateHertz: sampleRateHertz,
        languageCode: languageCode,
        alternativeLanguageCodes: ['en-IN', 'hi-IN'], // Fallback languages
        enableAutomaticPunctuation: true,
        enableWordTimeOffsets: false,
        model: 'latest_long', // Use latest model for better accuracy
        useEnhanced: true, // Enhanced model for better accuracy
        maxAlternatives: 1,
        speechContexts: [
          {
            phrases: [
              "farming", "crop", "disease", "pest", "fertilizer", "irrigation", "harvest", "seed", "soil", "weather",
              "wheat", "rice", "cotton", "tomato", "potato", "onion", "maize", "sugarcane", "millet",
              "yellowing", "wilting", "spots", "insects", "fungus", "drought", "water", "market", "price"
            ],
            boost: 20.0
          }
        ]
      },
    };

    console.log(`üé§ Processing speech in ${SUPPORTED_LANGUAGES[languageCode] || languageCode}...`);
    console.log(`üìä Audio buffer size: ${audioBuffer.length} bytes`);
    
    const [response] = await speechClient.recognize(request);
    
    if (!response.results || response.results.length === 0) {
      console.log('‚ö†Ô∏è No speech detected in audio');
      throw new Error('No speech detected in the audio. Please speak clearly and try again.');
    }
    
    const transcription = response.results
      .map(result => result.alternatives[0].transcript)
      .join('\n')
      .trim();

    if (!transcription || transcription.length === 0) {
      console.log('‚ö†Ô∏è Empty transcription received');
      throw new Error('Could not understand the speech. Please speak clearly and try again.');
    }

    console.log(`üìù Transcribed text: "${transcription}"`);
    console.log(`üéØ Confidence: ${response.results[0]?.alternatives[0]?.confidence || 'N/A'}`);
    
    return transcription;
  } catch (error) {
    console.error('Speech-to-Text error:', error);
    
    // Enhanced error handling
    if (error.code === 3) { // INVALID_ARGUMENT
      throw new Error('Audio format not supported. Please check your microphone settings.');
    } else if (error.code === 7) { // PERMISSION_DENIED
      throw new Error('Speech recognition service not available. Using demo mode.');
    } else if (error.code === 14) { // UNAVAILABLE
      throw new Error('Speech recognition service temporarily unavailable.');
    }
    
    throw new Error(`Speech recognition failed: ${error.message}`);
  }
}

/**
 * Convert text to speech using Vertex AI Text-to-Speech
 * @param {string} text - Text to convert to speech
 * @param {string} languageCode - Language code for output speech
 * @param {string} voiceName - Specific voice name (optional)
 * @param {string} gender - Voice gender ('NEUTRAL', 'MALE', 'FEMALE')
 * @returns {Promise<Buffer>} Audio buffer
 */
async function convertTextToSpeech(text, languageCode = 'hi-IN', voiceName = null, gender = 'NEUTRAL') {
  try {
    // Voice configuration based on language
    const voiceConfig = {
      languageCode: languageCode,
      ssmlGender: gender,
    };

    // Use specific voice if provided, otherwise let Google choose
    if (voiceName) {
      voiceConfig.name = voiceName;
    }

    const request = {
      input: { text: text },
      voice: voiceConfig,
      audioConfig: {
        audioEncoding: 'MP3',
        speakingRate: 0.9, // Slightly slower for better comprehension
        pitch: 0.0,
        volumeGainDb: 0.0,
      },
    };

    console.log(`üîä Generating speech in ${SUPPORTED_LANGUAGES[languageCode] || languageCode}...`);
    
    const [response] = await ttsClient.synthesizeSpeech(request);
    console.log('‚úÖ Speech generated successfully');
    
    return response.audioContent;
  } catch (error) {
    console.error('Text-to-Speech error:', error);
    throw new Error(`Speech synthesis failed: ${error.message}`);
  }
}

/**
 * Process farmer's voice query using AI and return voice response
 * @param {Buffer} audioBuffer - Input audio from farmer
 * @param {string} inputLanguage - Farmer's language
 * @param {object} context - Additional context (location, crop, etc.)
 * @returns {Promise<{text: string, audio: Buffer, language: string}>}
 */
async function processVoiceQuery(audioBuffer, inputLanguage = 'hi-IN', context = {}) {
  try {
    // Step 1: Convert speech to text
    const userQuery = await speechToText(audioBuffer, inputLanguage);
    
    if (!userQuery || userQuery.trim().length === 0) {
      throw new Error('Could not understand the audio. Please try again.');
    }

    // Step 2: Create enhanced prompt for AI with agricultural context
    const enhancedPrompt = createAgriculturalPrompt(userQuery, context, inputLanguage);
    
    // Step 3: Get AI response using Vertex AI
    console.log('ü§ñ Processing with Vertex AI...');
    const result = await generativeModel.generateContent(enhancedPrompt);
    const aiResponse = result.response.candidates[0].content.parts[0].text;
    
    console.log(`üí¨ AI Response: ${aiResponse}`);

    // Step 4: Convert AI response back to speech
    const responseAudio = await convertTextToSpeech(aiResponse, inputLanguage);
    
    return {
      userQuery: userQuery,
      text: aiResponse,
      audio: responseAudio,
      language: inputLanguage,
      timestamp: new Date().toISOString()
    };
    
  } catch (error) {
    console.error('Voice query processing error:', error);
    
    // Generate error message in user's language
    const errorMessage = getErrorMessage(inputLanguage);
    const errorAudio = await convertTextToSpeech(errorMessage, inputLanguage);
    
    return {
      userQuery: '',
      text: errorMessage,
      audio: errorAudio,
      language: inputLanguage,
      error: true,
      timestamp: new Date().toISOString()
    };
  }
}

/**
 * Create agricultural-focused prompt for better AI responses
 */
function createAgriculturalPrompt(userQuery, context, languageCode) {
  const language = SUPPORTED_LANGUAGES[languageCode] || 'English';
  
  return `You are "FarmMitra" (‡§´‡§æ‡§∞‡•ç‡§Æ ‡§Æ‡§ø‡§§‡•ç‡§∞), an expert agricultural advisor with 20+ years of experience helping Indian farmers. You understand:

üåæ CROPS: Wheat, Rice, Cotton, Sugarcane, Corn, Vegetables, Fruits, Pulses, Oilseeds
üêõ PESTS: Bollworm, Aphids, Whitefly, Stem borer, Leaf curl virus, Bacterial blight
üíä TREATMENTS: Neem oil, Copper fungicide, Imidacloprid, Carbendazim, Bio-pesticides
üå± SEASONS: Kharif (June-Oct), Rabi (Nov-April), Zaid (April-June)
üåßÔ∏è WEATHER: Monsoon patterns, drought management, flood protection
üí∞ SCHEMES: PM-KISAN, Crop insurance, Soil health cards, DBT

FARMER CONTEXT:
Location: ${context.location || 'India'} | Season: ${context.season || 'Current'} | Crop: ${context.crop || 'Mixed farming'} | Experience: ${context.experience || 'Moderate'}

FARMER'S QUESTION: "${userQuery}"

ANALYSIS: First, identify if this is about:
- Pest/Disease (symptoms, treatment, prevention)
- Crop Management (planting, irrigation, harvesting)
- Market/Prices (selling, buying, storage)
- Weather/Climate (rain, drought, temperature)
- Government Schemes (subsidies, loans, insurance)
- General Farming (seeds, fertilizers, equipment)

RESPONSE GUIDELINES:
‚úÖ Use ONLY ${language} language - simple, farmer-friendly words
‚úÖ Give specific, actionable steps with quantities/timing
‚úÖ Mention cost-effective solutions available locally
‚úÖ Include preventive measures for future
‚úÖ Reference local agricultural practices
‚úÖ Keep response practical and encouraging (100-150 words)

EXAMPLE RESPONSE STRUCTURE:
"‡§Æ‡•à‡§Ç ‡§∏‡§Æ‡§ù ‡§ó‡§Ø‡§æ ‡§Ü‡§™‡§ï‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ‡•§ [Problem acknowledgment]
‡§Ø‡§π [specific issue] ‡§ï‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§π‡•à‡•§ [Diagnosis]
‡§§‡•Å‡§∞‡§Ç‡§§ ‡§ï‡§∞‡•á‡§Ç: [Immediate action 1], [Immediate action 2]
‡§â‡§™‡§ö‡§æ‡§∞: [Treatment with dosage] 
‡§∞‡•ã‡§ï‡§•‡§æ‡§Æ: [Prevention method]
‡§≤‡§æ‡§ó‡§§: ‡§≤‡§ó‡§≠‡§ó [cost estimate]
‡§ï‡§¨ ‡§§‡§ï ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ: [timeline]
‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™‡§®‡•á [follow-up question]?"

Now respond as FarmMitra in ${language}:`;
}

/**
 * Get error messages in different languages
 */
function getErrorMessage(languageCode) {
  const errorMessages = {
    'hi-IN': '‡§Æ‡§æ‡§´ ‡§ï‡§∞‡•á‡§Ç, ‡§Æ‡•Å‡§ù‡•á ‡§Ü‡§™‡§ï‡•Ä ‡§¨‡§æ‡§§ ‡§∏‡§Æ‡§ù‡§®‡•á ‡§Æ‡•á‡§Ç ‡§ï‡•Å‡§õ ‡§™‡§∞‡•á‡§∂‡§æ‡§®‡•Ä ‡§π‡•Å‡§à ‡§π‡•à‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§¶‡•ã‡§¨‡§æ‡§∞‡§æ ‡§ï‡•ã‡§∂‡§ø‡§∂ ‡§ï‡§∞‡•á‡§Ç‡•§',
    'en-IN': 'Sorry, I had trouble understanding you. Please try again.',
    'bn-IN': '‡¶¶‡ßÅ‡¶É‡¶ñ‡¶ø‡¶§, ‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶ï‡¶•‡¶æ ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶Ö‡¶®‡ßÅ‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡ßá ‡¶Ü‡¶¨‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®‡•§',
    'te-IN': '‡∞ï‡±ç‡∞∑‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø, ‡∞Æ‡±Ä ‡∞Æ‡∞æ‡∞ü ‡∞Ö‡∞∞‡±ç‡∞•‡∞Ç ‡∞ö‡±á‡∞∏‡±Å‡∞ï‡±ã‡∞µ‡∞°‡∞Ç‡∞≤‡±ã ‡∞®‡∞æ‡∞ï‡±Å ‡∞á‡∞¨‡±ç‡∞¨‡∞Ç‡∞¶‡∞ø ‡∞â‡∞Ç‡∞¶‡∞ø. ‡∞¶‡∞Ø‡∞ö‡±á‡∞∏‡∞ø ‡∞Æ‡∞≥‡±ç‡∞≤‡±Ä ‡∞™‡±ç‡∞∞‡∞Ø‡∞§‡±ç‡∞®‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø‡•§',
    'mr-IN': '‡§Æ‡§æ‡§´ ‡§ï‡§∞‡§æ, ‡§Æ‡§≤‡§æ ‡§§‡•Å‡§Æ‡§ö‡•á ‡§Æ‡•ç‡§π‡§£‡§£‡•á ‡§∏‡§Æ‡§ú‡§£‡•ç‡§Ø‡§æ‡§§ ‡§Ö‡§°‡§ö‡§£ ‡§Ü‡§≤‡•Ä ‡§Ü‡§π‡•á. ‡§ï‡•É‡§™‡§Ø‡§æ ‡§™‡•Å‡§®‡•ç‡§π‡§æ ‡§™‡•ç‡§∞‡§Ø‡§§‡•ç‡§® ‡§ï‡§∞‡§æ.',
    'ta-IN': '‡ÆÆ‡Æ©‡Øç‡Æ©‡Æø‡Æï‡Øç‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç, ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æ™‡Øá‡Æö‡Øç‡Æö‡Øà‡Æ™‡Øç ‡Æ™‡ØÅ‡Æ∞‡Æø‡Æ®‡Øç‡Æ§‡ØÅ‡Æï‡Øä‡Æ≥‡Øç‡Æµ‡Æ§‡Æø‡Æ≤‡Øç ‡Æé‡Æ©‡Æï‡Øç‡Æï‡ØÅ ‡Æö‡Æø‡Æ∞‡ÆÆ‡ÆÆ‡Øç ‡Æè‡Æ±‡Øç‡Æ™‡Æü‡Øç‡Æü‡Æ§‡ØÅ. ‡Æ§‡ÆØ‡Æµ‡ØÅ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡ØÅ ‡ÆÆ‡ØÄ‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç ‡ÆÆ‡ØÅ‡ÆØ‡Æ±‡Øç‡Æö‡Æø‡Æï‡Øç‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç.',
    'gu-IN': '‡™Æ‡™æ‡™´ ‡™ï‡™∞‡™∂‡´ã, ‡™Æ‡™®‡´á ‡™§‡™Æ‡™æ‡™∞‡´Ä ‡™µ‡™æ‡™§ ‡™∏‡™Æ‡™ú‡™µ‡™æ‡™Æ‡™æ‡™Ç ‡™Æ‡´Å‡™∂‡´ç‡™ï‡´á‡™≤‡´Ä ‡™™‡™°‡´Ä ‡™õ‡´á. ‡™ï‡´É‡™™‡™æ ‡™ï‡™∞‡´Ä‡™®‡´á ‡™´‡™∞‡´Ä‡™•‡´Ä ‡™™‡´ç‡™∞‡™Ø‡™æ‡™∏ ‡™ï‡™∞‡´ã.',
    'kn-IN': '‡≤ï‡≥ç‡≤∑‡≤Æ‡≤ø‡≤∏‡≤ø, ‡≤®‡≤ø‡≤Æ‡≥ç‡≤Æ ‡≤Æ‡≤æ‡≤§‡≤®‡≥ç‡≤®‡≥Å ‡≤Ö‡≤∞‡≥ç‡≤• ‡≤Æ‡≤æ‡≤°‡≤ø‡≤ï‡≥ä‡≤≥‡≥ç‡≤≥‡≥Å‡≤µ‡≤≤‡≥ç‡≤≤‡≤ø ‡≤®‡≤®‡≤ó‡≥Ü ‡≤§‡≥ä‡≤Ç‡≤¶‡≤∞‡≥Ü ‡≤Ü‡≤ó‡≤ø‡≤¶‡≥Ü. ‡≤¶‡≤Ø‡≤µ‡≤ø‡≤ü‡≥ç‡≤ü‡≥Å ‡≤Æ‡≤§‡≥ç‡≤§‡≥Ü ‡≤™‡≥ç‡≤∞‡≤Ø‡≤§‡≥ç‡≤®‡≤ø‡≤∏‡≤ø.',
    'ml-IN': '‡¥ï‡µç‡¥∑‡¥Æ‡¥ø‡¥ï‡µç‡¥ï‡¥£‡¥Ç, ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥∏‡¥Ç‡¥∏‡¥æ‡¥∞‡¥Ç ‡¥Æ‡¥®‡¥∏‡µç‡¥∏‡¥ø‡¥≤‡¥æ‡¥ï‡µç‡¥ï‡¥æ‡µª ‡¥é‡¥®‡¥ø‡¥ï‡µç‡¥ï‡µç ‡¥¨‡µÅ‡¥¶‡µç‡¥ß‡¥ø‡¥Æ‡µÅ‡¥ü‡µç‡¥ü‡µç ‡¥â‡¥£‡µç‡¥ü‡¥æ‡¥Ø‡¥ø. ‡¥¶‡¥Ø‡¥µ‡¥æ‡¥Ø‡¥ø ‡¥µ‡µÄ‡¥£‡µç‡¥ü‡µÅ‡¥Ç ‡¥∂‡µç‡¥∞‡¥Æ‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï.',
    'pa-IN': '‡®Æ‡®æ‡®´‡®º ‡®ï‡®∞‡®®‡®æ, ‡®Æ‡©à‡®®‡©Ç‡©∞ ‡®§‡©Å‡®π‡®æ‡®°‡©Ä ‡®ó‡©±‡®≤ ‡®∏‡®Æ‡®ù‡®£ ‡®µ‡®ø‡©±‡®ö ‡®Æ‡©Å‡®∏‡®º‡®ï‡®≤ ‡®π‡©ã‡®à ‡®π‡©à‡•§ ‡®ï‡®ø‡®∞‡®™‡®æ ‡®ï‡®∞‡®ï‡©á ‡®¶‡©Å‡®¨‡®æ‡®∞‡®æ ‡®ï‡©ã‡®∏‡®º‡®ø‡®∏‡®º ‡®ï‡®∞‡©ã‡•§',
    'or-IN': '‡¨¶‡≠Å‡¨É‡¨ñ‡¨ø‡¨§, ‡¨Æ‡≠Å‡¨Å ‡¨Ü‡¨™‡¨£‡¨ô‡≠ç‡¨ï ‡¨ï‡¨•‡¨æ ‡¨¨‡≠Å‡¨ù‡¨ø‡¨¨‡¨æ‡¨∞‡≠á ‡¨Ö‡¨∏‡≠Å‡¨¨‡¨ø‡¨ß‡¨æ ‡¨≠‡≠ã‡¨ó‡≠Å‡¨õ‡¨ø‡•§ ‡¨¶‡≠ü‡¨æ‡¨ï‡¨∞‡¨ø ‡¨™‡≠Å‡¨®‡¨∞‡≠ç‡¨¨‡¨æ‡¨∞ ‡¨ö‡≠á‡¨∑‡≠ç‡¨ü‡¨æ ‡¨ï‡¨∞‡¨®‡≠ç‡¨§‡≠Å‡•§'
  };
  
  return errorMessages[languageCode] || errorMessages['en-IN'];
}

/**
 * Get available voices for a language
 */
async function getAvailableVoices(languageCode) {
  try {
    const [result] = await ttsClient.listVoices({ languageCode });
    return result.voices.filter(voice => voice.languageCodes.includes(languageCode));
  } catch (error) {
    console.error('Error fetching voices:', error);
    return [];
  }
}

/**
 * Save voice interaction to file for debugging/logging
 */
async function saveVoiceInteraction(interaction, filename) {
  try {
    const logsDir = path.join(__dirname, '../logs');
    if (!fs.existsSync(logsDir)) {
      fs.mkdirSync(logsDir, { recursive: true });
    }
    
    const logFile = path.join(logsDir, `voice_${filename}_${Date.now()}.json`);
    fs.writeFileSync(logFile, JSON.stringify(interaction, null, 2));
    
    if (interaction.audio) {
      const audioFile = path.join(logsDir, `voice_${filename}_${Date.now()}.mp3`);
      fs.writeFileSync(audioFile, interaction.audio);
    }
  } catch (error) {
    console.error('Error saving voice interaction:', error);
  }
}

export {
  speechToText,
  convertTextToSpeech as textToSpeech,
  processVoiceQuery,
  getAvailableVoices,
  saveVoiceInteraction,
  SUPPORTED_LANGUAGES
};
