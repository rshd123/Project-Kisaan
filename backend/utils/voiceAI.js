// utils/voiceAI.js - Voice-first interaction using Vertex AI Speech services
import speech from '@google-cloud/speech';
import textToSpeech from '@google-cloud/text-to-speech';
import { generativeModel } from './vertex.js';
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';
import dotenv from 'dotenv';

dotenv.config();

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Initialize Speech-to-Text client
const speechClient = new speech.SpeechClient();

// Initialize Text-to-Speech client
const ttsClient = new textToSpeech.TextToSpeechClient();

// Supported languages/dialects for Indian farmers
const SUPPORTED_LANGUAGES = {
  'hi-IN': 'Hindi',
  'en-IN': 'English (India)',
  'bn-IN': 'Bengali',
  'te-IN': 'Telugu',
  'mr-IN': 'Marathi',
  'ta-IN': 'Tamil',
  'gu-IN': 'Gujarati',
  'kn-IN': 'Kannada',
  'ml-IN': 'Malayalam',
  'pa-IN': 'Punjabi',
  'or-IN': 'Odia'
};

/**
 * Convert speech audio buffer to text using Vertex AI Speech-to-Text
 * @param {Buffer} audioBuffer - Audio file buffer
 * @param {string} languageCode - Language code (e.g., 'hi-IN', 'en-IN')
 * @param {string} encoding - Audio encoding (e.g., 'WEBM_OPUS', 'MP3', 'WAV')
 * @param {number} sampleRateHertz - Sample rate of audio
 * @returns {Promise<string>} Transcribed text
 */
async function speechToText(audioBuffer, languageCode = 'hi-IN', encoding = 'WEBM_OPUS', sampleRateHertz = 48000) {
  try {
    const request = {
      audio: {
        content: audioBuffer.toString('base64'),
      },
      config: {
        encoding: encoding,
        sampleRateHertz: sampleRateHertz,
        languageCode: languageCode,
        alternativeLanguageCodes: ['en-IN', 'hi-IN'], // Fallback languages
        enableAutomaticPunctuation: true,
        enableWordTimeOffsets: false,
        model: 'latest_long', // Use latest model for better accuracy
        useEnhanced: true, // Enhanced model for better accuracy
      },
    };

    console.log(`üé§ Processing speech in ${SUPPORTED_LANGUAGES[languageCode] || languageCode}...`);
    
    const [response] = await speechClient.recognize(request);
    const transcription = response.results
      .map(result => result.alternatives[0].transcript)
      .join('\n');

    console.log(`üìù Transcribed text: ${transcription}`);
    return transcription;
  } catch (error) {
    console.error('Speech-to-Text error:', error);
    throw new Error(`Speech recognition failed: ${error.message}`);
  }
}

/**
 * Convert text to speech using Vertex AI Text-to-Speech
 * @param {string} text - Text to convert to speech
 * @param {string} languageCode - Language code for output speech
 * @param {string} voiceName - Specific voice name (optional)
 * @param {string} gender - Voice gender ('NEUTRAL', 'MALE', 'FEMALE')
 * @returns {Promise<Buffer>} Audio buffer
 */
async function convertTextToSpeech(text, languageCode = 'hi-IN', voiceName = null, gender = 'NEUTRAL') {
  try {
    // Voice configuration based on language
    const voiceConfig = {
      languageCode: languageCode,
      ssmlGender: gender,
    };

    // Use specific voice if provided, otherwise let Google choose
    if (voiceName) {
      voiceConfig.name = voiceName;
    }

    const request = {
      input: { text: text },
      voice: voiceConfig,
      audioConfig: {
        audioEncoding: 'MP3',
        speakingRate: 0.9, // Slightly slower for better comprehension
        pitch: 0.0,
        volumeGainDb: 0.0,
      },
    };

    console.log(`üîä Generating speech in ${SUPPORTED_LANGUAGES[languageCode] || languageCode}...`);
    
    const [response] = await ttsClient.synthesizeSpeech(request);
    console.log('‚úÖ Speech generated successfully');
    
    return response.audioContent;
  } catch (error) {
    console.error('Text-to-Speech error:', error);
    throw new Error(`Speech synthesis failed: ${error.message}`);
  }
}

/**
 * Process farmer's voice query using AI and return voice response
 * @param {Buffer} audioBuffer - Input audio from farmer
 * @param {string} inputLanguage - Farmer's language
 * @param {object} context - Additional context (location, crop, etc.)
 * @returns {Promise<{text: string, audio: Buffer, language: string}>}
 */
async function processVoiceQuery(audioBuffer, inputLanguage = 'hi-IN', context = {}) {
  try {
    // Step 1: Convert speech to text
    const userQuery = await speechToText(audioBuffer, inputLanguage);
    
    if (!userQuery || userQuery.trim().length === 0) {
      throw new Error('Could not understand the audio. Please try again.');
    }

    // Step 2: Create enhanced prompt for AI with agricultural context
    const enhancedPrompt = createAgriculturalPrompt(userQuery, context, inputLanguage);
    
    // Step 3: Get AI response using Vertex AI
    console.log('ü§ñ Processing with Vertex AI...');
    const result = await generativeModel.generateContent(enhancedPrompt);
    const aiResponse = result.response.text();
    
    console.log(`üí¨ AI Response: ${aiResponse}`);

    // Step 4: Convert AI response back to speech
    const responseAudio = await convertTextToSpeech(aiResponse, inputLanguage);
    
    return {
      userQuery: userQuery,
      text: aiResponse,
      audio: responseAudio,
      language: inputLanguage,
      timestamp: new Date().toISOString()
    };
    
  } catch (error) {
    console.error('Voice query processing error:', error);
    
    // Generate error message in user's language
    const errorMessage = getErrorMessage(inputLanguage);
    const errorAudio = await convertTextToSpeech(errorMessage, inputLanguage);
    
    return {
      userQuery: '',
      text: errorMessage,
      audio: errorAudio,
      language: inputLanguage,
      error: true,
      timestamp: new Date().toISOString()
    };
  }
}

/**
 * Create agricultural-focused prompt for better AI responses
 */
function createAgriculturalPrompt(userQuery, context, languageCode) {
  const language = SUPPORTED_LANGUAGES[languageCode] || 'English';
  
  return `You are an expert agricultural advisor helping Indian farmers. Respond in ${language} language.

Context Information:
- Farmer's Location: ${context.location || 'India'}
- Current Season: ${context.season || 'Current season'}
- Crop Interest: ${context.crop || 'General farming'}
- Farmer's Experience: ${context.experience || 'Varied'}

Farmer's Question: "${userQuery}"

Instructions:
1. Provide practical, actionable advice suitable for Indian farming conditions
2. Use simple, clear language that a farmer can easily understand
3. Include specific steps or recommendations when possible
4. If discussing crops, consider Indian climate and soil conditions
5. For market prices, mention checking local mandis or government sources
6. Keep response concise but helpful (under 150 words)
7. Be encouraging and supportive in tone
8. If you need more information, ask specific follow-up questions

Please respond in ${language}:`;
}

/**
 * Get error messages in different languages
 */
function getErrorMessage(languageCode) {
  const errorMessages = {
    'hi-IN': '‡§Æ‡§æ‡§´ ‡§ï‡§∞‡•á‡§Ç, ‡§Æ‡•Å‡§ù‡•á ‡§Ü‡§™‡§ï‡•Ä ‡§¨‡§æ‡§§ ‡§∏‡§Æ‡§ù‡§®‡•á ‡§Æ‡•á‡§Ç ‡§ï‡•Å‡§õ ‡§™‡§∞‡•á‡§∂‡§æ‡§®‡•Ä ‡§π‡•Å‡§à ‡§π‡•à‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§¶‡•ã‡§¨‡§æ‡§∞‡§æ ‡§ï‡•ã‡§∂‡§ø‡§∂ ‡§ï‡§∞‡•á‡§Ç‡•§',
    'en-IN': 'Sorry, I had trouble understanding you. Please try again.',
    'bn-IN': '‡¶¶‡ßÅ‡¶É‡¶ñ‡¶ø‡¶§, ‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶ï‡¶•‡¶æ ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶Ö‡¶®‡ßÅ‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡ßá ‡¶Ü‡¶¨‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®‡•§',
    'te-IN': '‡∞ï‡±ç‡∞∑‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø, ‡∞Æ‡±Ä ‡∞Æ‡∞æ‡∞ü ‡∞Ö‡∞∞‡±ç‡∞•‡∞Ç ‡∞ö‡±á‡∞∏‡±Å‡∞ï‡±ã‡∞µ‡∞°‡∞Ç‡∞≤‡±ã ‡∞®‡∞æ‡∞ï‡±Å ‡∞á‡∞¨‡±ç‡∞¨‡∞Ç‡∞¶‡∞ø ‡∞â‡∞Ç‡∞¶‡∞ø. ‡∞¶‡∞Ø‡∞ö‡±á‡∞∏‡∞ø ‡∞Æ‡∞≥‡±ç‡∞≤‡±Ä ‡∞™‡±ç‡∞∞‡∞Ø‡∞§‡±ç‡∞®‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø‡•§',
    'mr-IN': '‡§Æ‡§æ‡§´ ‡§ï‡§∞‡§æ, ‡§Æ‡§≤‡§æ ‡§§‡•Å‡§Æ‡§ö‡•á ‡§Æ‡•ç‡§π‡§£‡§£‡•á ‡§∏‡§Æ‡§ú‡§£‡•ç‡§Ø‡§æ‡§§ ‡§Ö‡§°‡§ö‡§£ ‡§Ü‡§≤‡•Ä ‡§Ü‡§π‡•á. ‡§ï‡•É‡§™‡§Ø‡§æ ‡§™‡•Å‡§®‡•ç‡§π‡§æ ‡§™‡•ç‡§∞‡§Ø‡§§‡•ç‡§® ‡§ï‡§∞‡§æ.',
    'ta-IN': '‡ÆÆ‡Æ©‡Øç‡Æ©‡Æø‡Æï‡Øç‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç, ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æ™‡Øá‡Æö‡Øç‡Æö‡Øà‡Æ™‡Øç ‡Æ™‡ØÅ‡Æ∞‡Æø‡Æ®‡Øç‡Æ§‡ØÅ‡Æï‡Øä‡Æ≥‡Øç‡Æµ‡Æ§‡Æø‡Æ≤‡Øç ‡Æé‡Æ©‡Æï‡Øç‡Æï‡ØÅ ‡Æö‡Æø‡Æ∞‡ÆÆ‡ÆÆ‡Øç ‡Æè‡Æ±‡Øç‡Æ™‡Æü‡Øç‡Æü‡Æ§‡ØÅ. ‡Æ§‡ÆØ‡Æµ‡ØÅ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡ØÅ ‡ÆÆ‡ØÄ‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç ‡ÆÆ‡ØÅ‡ÆØ‡Æ±‡Øç‡Æö‡Æø‡Æï‡Øç‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç.',
    'gu-IN': '‡™Æ‡™æ‡™´ ‡™ï‡™∞‡™∂‡´ã, ‡™Æ‡™®‡´á ‡™§‡™Æ‡™æ‡™∞‡´Ä ‡™µ‡™æ‡™§ ‡™∏‡™Æ‡™ú‡™µ‡™æ‡™Æ‡™æ‡™Ç ‡™Æ‡´Å‡™∂‡´ç‡™ï‡´á‡™≤‡´Ä ‡™™‡™°‡´Ä ‡™õ‡´á. ‡™ï‡´É‡™™‡™æ ‡™ï‡™∞‡´Ä‡™®‡´á ‡™´‡™∞‡´Ä‡™•‡´Ä ‡™™‡´ç‡™∞‡™Ø‡™æ‡™∏ ‡™ï‡™∞‡´ã.',
    'kn-IN': '‡≤ï‡≥ç‡≤∑‡≤Æ‡≤ø‡≤∏‡≤ø, ‡≤®‡≤ø‡≤Æ‡≥ç‡≤Æ ‡≤Æ‡≤æ‡≤§‡≤®‡≥ç‡≤®‡≥Å ‡≤Ö‡≤∞‡≥ç‡≤• ‡≤Æ‡≤æ‡≤°‡≤ø‡≤ï‡≥ä‡≤≥‡≥ç‡≤≥‡≥Å‡≤µ‡≤≤‡≥ç‡≤≤‡≤ø ‡≤®‡≤®‡≤ó‡≥Ü ‡≤§‡≥ä‡≤Ç‡≤¶‡≤∞‡≥Ü ‡≤Ü‡≤ó‡≤ø‡≤¶‡≥Ü. ‡≤¶‡≤Ø‡≤µ‡≤ø‡≤ü‡≥ç‡≤ü‡≥Å ‡≤Æ‡≤§‡≥ç‡≤§‡≥Ü ‡≤™‡≥ç‡≤∞‡≤Ø‡≤§‡≥ç‡≤®‡≤ø‡≤∏‡≤ø.',
    'ml-IN': '‡¥ï‡µç‡¥∑‡¥Æ‡¥ø‡¥ï‡µç‡¥ï‡¥£‡¥Ç, ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥∏‡¥Ç‡¥∏‡¥æ‡¥∞‡¥Ç ‡¥Æ‡¥®‡¥∏‡µç‡¥∏‡¥ø‡¥≤‡¥æ‡¥ï‡µç‡¥ï‡¥æ‡µª ‡¥é‡¥®‡¥ø‡¥ï‡µç‡¥ï‡µç ‡¥¨‡µÅ‡¥¶‡µç‡¥ß‡¥ø‡¥Æ‡µÅ‡¥ü‡µç‡¥ü‡µç ‡¥â‡¥£‡µç‡¥ü‡¥æ‡¥Ø‡¥ø. ‡¥¶‡¥Ø‡¥µ‡¥æ‡¥Ø‡¥ø ‡¥µ‡µÄ‡¥£‡µç‡¥ü‡µÅ‡¥Ç ‡¥∂‡µç‡¥∞‡¥Æ‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï.',
    'pa-IN': '‡®Æ‡®æ‡®´‡®º ‡®ï‡®∞‡®®‡®æ, ‡®Æ‡©à‡®®‡©Ç‡©∞ ‡®§‡©Å‡®π‡®æ‡®°‡©Ä ‡®ó‡©±‡®≤ ‡®∏‡®Æ‡®ù‡®£ ‡®µ‡®ø‡©±‡®ö ‡®Æ‡©Å‡®∏‡®º‡®ï‡®≤ ‡®π‡©ã‡®à ‡®π‡©à‡•§ ‡®ï‡®ø‡®∞‡®™‡®æ ‡®ï‡®∞‡®ï‡©á ‡®¶‡©Å‡®¨‡®æ‡®∞‡®æ ‡®ï‡©ã‡®∏‡®º‡®ø‡®∏‡®º ‡®ï‡®∞‡©ã‡•§',
    'or-IN': '‡¨¶‡≠Å‡¨É‡¨ñ‡¨ø‡¨§, ‡¨Æ‡≠Å‡¨Å ‡¨Ü‡¨™‡¨£‡¨ô‡≠ç‡¨ï ‡¨ï‡¨•‡¨æ ‡¨¨‡≠Å‡¨ù‡¨ø‡¨¨‡¨æ‡¨∞‡≠á ‡¨Ö‡¨∏‡≠Å‡¨¨‡¨ø‡¨ß‡¨æ ‡¨≠‡≠ã‡¨ó‡≠Å‡¨õ‡¨ø‡•§ ‡¨¶‡≠ü‡¨æ‡¨ï‡¨∞‡¨ø ‡¨™‡≠Å‡¨®‡¨∞‡≠ç‡¨¨‡¨æ‡¨∞ ‡¨ö‡≠á‡¨∑‡≠ç‡¨ü‡¨æ ‡¨ï‡¨∞‡¨®‡≠ç‡¨§‡≠Å‡•§'
  };
  
  return errorMessages[languageCode] || errorMessages['en-IN'];
}

/**
 * Get available voices for a language
 */
async function getAvailableVoices(languageCode) {
  try {
    const [result] = await ttsClient.listVoices({ languageCode });
    return result.voices.filter(voice => voice.languageCodes.includes(languageCode));
  } catch (error) {
    console.error('Error fetching voices:', error);
    return [];
  }
}

/**
 * Save voice interaction to file for debugging/logging
 */
async function saveVoiceInteraction(interaction, filename) {
  try {
    const logsDir = path.join(__dirname, '../logs');
    if (!fs.existsSync(logsDir)) {
      fs.mkdirSync(logsDir, { recursive: true });
    }
    
    const logFile = path.join(logsDir, `voice_${filename}_${Date.now()}.json`);
    fs.writeFileSync(logFile, JSON.stringify(interaction, null, 2));
    
    if (interaction.audio) {
      const audioFile = path.join(logsDir, `voice_${filename}_${Date.now()}.mp3`);
      fs.writeFileSync(audioFile, interaction.audio);
    }
  } catch (error) {
    console.error('Error saving voice interaction:', error);
  }
}

export {
  speechToText,
  convertTextToSpeech as textToSpeech,
  processVoiceQuery,
  getAvailableVoices,
  saveVoiceInteraction,
  SUPPORTED_LANGUAGES
};
